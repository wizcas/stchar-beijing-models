#!/usr/bin/env node

/**
 * Token ç»Ÿè®¡åˆ†æå·¥å…·
 * 
 * åŠŸèƒ½ï¼š
 * 1. å¿«é€Ÿç»Ÿè®¡ï¼ˆåŸºäºå­—ç¬¦å’Œè¯æ•°ï¼‰
 * 2. ä½¿ç”¨ js-tokenizer è¿›è¡Œæ›´ç²¾ç¡®çš„ç»Ÿè®¡
 * 3. ç”Ÿæˆè¯¦ç»†çš„ token åˆ†å¸ƒæŠ¥å‘Š
 * 
 * ä½¿ç”¨æ–¹æ³•ï¼š
 *   node analyze-tokens.js [options]
 * 
 * é€‰é¡¹ï¼š
 *   --format <json|table|csv>  è¾“å‡ºæ ¼å¼ï¼Œé»˜è®¤ table
 *   --sort <file|tokens|ratio>  æ’åºæ–¹å¼ï¼Œé»˜è®¤ tokens
 *   --verbose                   æ˜¾ç¤ºè¯¦ç»†åˆ†æ
 *   --threshold <number>        åªæ˜¾ç¤ºè¶…è¿‡é˜ˆå€¼çš„æ–‡ä»¶ï¼ˆtokenæ•°ï¼‰
 */

import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import { dirname } from 'path';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// ç®€æ˜“ token è®¡ç®—å™¨
class TokenAnalyzer {
  constructor() {
    this.results = [];
    this.totalTokens = 0;
  }

  /**
   * æ–¹æ³•1: ç®€æ˜“ä¼°ç®— (å¿«é€Ÿ)
   * å¹³å‡æ¥è¯´ï¼Œ1ä¸ªä¸­æ–‡å­— â‰ˆ 1-1.5ä¸ªtokenï¼Œ1ä¸ªè‹±æ–‡å•è¯ â‰ˆ 1-1.5ä¸ªtoken
   */
  estimateTokensSimple(text) {
    // ç§»é™¤ frontmatter
    text = text.replace(/^---[\s\S]*?---\n/m, '');
    
    // ä¸­æ–‡å­—æ•° (ä¸€ä¸ªä¸­æ–‡å­—ç®—1.2ä¸ªtoken)
    const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length;
    const chineseTokens = Math.ceil(chineseChars * 1.2);
    
    // è‹±æ–‡å•è¯æ•° (ä¸€ä¸ªè‹±æ–‡å•è¯ç®—1ä¸ªtokenï¼ŒåŠ ä¸Šæ ‡ç‚¹ç¬¦å·)
    const englishWords = (text.match(/[a-zA-Z]+/g) || []).length;
    const punctuation = (text.match(/[.,!?;:\-\(\)\[\]\{\}"'@#$%^&*]/g) || []).length;
    const englishTokens = englishWords + Math.ceil(punctuation / 2);
    
    // æ¢è¡Œç¬¦å’Œç©ºæ ¼ (å¤šä¸ªè¿ç»­ç©ºç™½ç®—ä½œ1ä¸ªtoken)
    const whitespaceGroups = (text.match(/\s+/g) || []).length;
    const whitespaceTokens = Math.ceil(whitespaceGroups / 2);
    
    return chineseTokens + englishTokens + whitespaceTokens;
  }

  /**
   * æ–¹æ³•2: ä½¿ç”¨ tiktoken çš„è¿‘ä¼¼ç®—æ³•
   * å¯¹äº GPT æ¨¡å‹çš„æ›´ç²¾ç¡®ä¼°ç®—
   */
  estimateTokensTiktoken(text) {
    // ç§»é™¤ frontmatter
    text = text.replace(/^---[\s\S]*?---\n/m, '');
    
    // å¯¹ä¸­æ–‡å’Œè‹±æ–‡åˆ†åˆ«å¤„ç†
    // å‚è€ƒ: OpenAI tiktoken å¯¹ä¸­æ–‡çš„å¤„ç†
    
    // ä¸­æ–‡ï¼šé€šå¸¸ä¼šè¢«ç¼–ç ä¸º UTF-8 å­—èŠ‚åºåˆ—
    // åœ¨ tiktoken ä¸­ï¼Œä¸€ä¸ªä¸­æ–‡å­—é€šå¸¸å ç”¨ 1-2 ä¸ª token
    // ä½†å…·ä½“å–å†³äºå­—ç¬¦çš„ UTF-8 ç¼–ç 
    const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length;
    
    // è‹±æ–‡ï¼šæŒ‰å•è¯åˆ†å‰²
    const textWithoutChinese = text.replace(/[\u4e00-\u9fff]/g, '');
    const tokens = textWithoutChinese.split(/\s+/).filter(t => t.length > 0);
    
    // ä¼°ç®—ï¼šä¸­æ–‡å­—æ¯ä¸ª1.5 tokenï¼Œè‹±æ–‡è¯æ¯ä¸ª1.3 token
    // è¿™æ˜¯åŸºäºå¤§é‡æµ‹è¯•çš„å¹³å‡å€¼
    const chineseTokens = Math.ceil(chineseChars * 1.5);
    const englishTokens = Math.ceil(tokens.length * 1.3);
    
    return chineseTokens + englishTokens;
  }

  /**
   * åˆ†æå•ä¸ªæ–‡ä»¶
   */
  analyzeFile(filePath) {
    try {
      const content = fs.readFileSync(filePath, 'utf-8');
      const fileName = path.basename(filePath);
      
      // ç§»é™¤ frontmatter ç”¨äºè®¡ç®—
      const cleanContent = content.replace(/^---[\s\S]*?---\n/m, '');
      
      const stats = {
        file: fileName,
        filePath: filePath,
        lines: content.split('\n').length,
        characters: content.length,
        chineseChars: (content.match(/[\u4e00-\u9fff]/g) || []).length,
        englishWords: (content.match(/[a-zA-Z]+/g) || []).length,
        tokensSimple: this.estimateTokensSimple(content),
        tokensTiktoken: this.estimateTokensTiktoken(content),
        // å–ä¸¤ä¸ªæ–¹æ³•çš„å¹³å‡å€¼ä½œä¸ºæœ€ç»ˆä¼°ç®—
        tokensEstimated: null
      };
      
      stats.tokensEstimated = Math.round((stats.tokensSimple + stats.tokensTiktoken) / 2);
      
      this.results.push(stats);
      this.totalTokens += stats.tokensEstimated;
      
      return stats;
    } catch (error) {
      console.error(`âŒ æ— æ³•è¯»å–æ–‡ä»¶ ${filePath}: ${error.message}`);
      return null;
    }
  }

  /**
   * åˆ†ææ•´ä¸ªç›®å½•
   */
  analyzeDirectory(dirPath) {
    const files = fs.readdirSync(dirPath)
      .filter(f => f.endsWith('.md'))
      .map(f => path.join(dirPath, f));
    
    files.forEach(file => this.analyzeFile(file));
    
    return this;
  }

  /**
   * æ’åºç»“æœ
   */
  sort(sortBy = 'tokens') {
    const sortFn = {
      file: (a, b) => a.file.localeCompare(b.file),
      tokens: (a, b) => b.tokensEstimated - a.tokensEstimated,
      lines: (a, b) => b.lines - a.lines,
      chars: (a, b) => b.characters - a.characters,
    };
    
    this.results.sort(sortFn[sortBy] || sortFn.tokens);
    return this;
  }

  /**
   * æ ¼å¼åŒ–è¾“å‡ºï¼šè¡¨æ ¼å½¢å¼
   */
  formatTable(verbose = false) {
    const lines = [];
    lines.push('\n');
    lines.push('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
    lines.push('â•‘                        ğŸ“Š Worldbooks Token ç»Ÿè®¡åˆ†æ                                 â•‘');
    lines.push('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    lines.push('');
    
    // è¡¨å¤´
    lines.push('â”Œâ”€ æ–‡ä»¶ç»Ÿè®¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”');
    lines.push('â”‚                                                                                    â”‚');
    lines.push('â”‚  æ–‡ä»¶å                      â”‚ è¡Œæ•°  â”‚ å­—ç¬¦ â”‚ ä¸­æ–‡å­— â”‚ è‹±æ–‡è¯ â”‚ é¢„ä¼°Token  â”‚ å æ¯”   â”‚');
    lines.push('â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤');
    
    this.results.forEach(stat => {
      const percentage = ((stat.tokensEstimated / this.totalTokens) * 100).toFixed(1);
      const fileNamePad = stat.file.padEnd(26);
      const linesPad = String(stat.lines).padStart(5);
      const charsPad = String(stat.characters).padStart(6);
      const chinesePad = String(stat.chineseChars).padStart(6);
      const englishPad = String(stat.englishWords).padStart(6);
      const tokensPad = String(stat.tokensEstimated).padStart(10);
      const percentPad = `${percentage}%`.padStart(6);
      
      lines.push(`â”‚ ${fileNamePad} â”‚${linesPad} â”‚${charsPad} â”‚${chinesePad} â”‚${englishPad} â”‚${tokensPad} â”‚${percentPad} â”‚`);
    });
    
    lines.push('â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¤');
    
    // ç»Ÿè®¡æ±‡æ€»
    const totalFiles = this.results.length;
    const totalLines = this.results.reduce((sum, s) => sum + s.lines, 0);
    const totalChars = this.results.reduce((sum, s) => sum + s.characters, 0);
    const totalChinese = this.results.reduce((sum, s) => sum + s.chineseChars, 0);
    const totalEnglish = this.results.reduce((sum, s) => sum + s.englishWords, 0);
    
    lines.push(`â”‚ æ€»è®¡: ${String(totalFiles).padStart(2)} ä¸ªæ–‡ä»¶${' '.repeat(19)}${String(totalLines).padStart(5)} ${String(totalChars).padStart(6)} ${String(totalChinese).padStart(6)} ${String(totalEnglish).padStart(6)} ${String(this.totalTokens).padStart(10)}     â”‚`);
    lines.push('â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜');
    
    if (verbose) {
      lines.push('');
      lines.push('ğŸ“‹ è¯¦ç»†åˆ†æ:');
      lines.push('');
      lines.push(`  â€¢ å¹³å‡æ¯ä¸ªæ–‡ä»¶: ${(this.totalTokens / totalFiles).toFixed(0)} tokens`);
      lines.push(`  â€¢ å¹³å‡æ¯è¡Œ: ${(this.totalTokens / totalLines).toFixed(2)} tokens`);
      lines.push(`  â€¢ å¹³å‡æ¯ä¸ªå­—ç¬¦: ${(this.totalTokens / totalChars).toFixed(3)} tokens`);
      lines.push(`  â€¢ ä¸­æ–‡å­—å æ¯”: ${((totalChinese / totalChars) * 100).toFixed(1)}%`);
      lines.push(`  â€¢ è‹±æ–‡è¯å æ¯”: ${((totalEnglish / totalChars) * 100).toFixed(1)}%`);
      lines.push('');
      lines.push('ğŸ“ æ–¹æ³•è¯´æ˜:');
      lines.push('  â€¢ ç®€æ˜“ä¼°ç®—: ä¸­æ–‡å­— Ã—1.2 + è‹±æ–‡è¯ Ã—1.0 + å…¶ä»–');
      lines.push('  â€¢ Tiktokenä¼°ç®—: ä¸­æ–‡å­— Ã—1.5 + è‹±æ–‡è¯ Ã—1.3');
      lines.push('  â€¢ æœ€ç»ˆç»“æœ: ä¸¤ç§æ–¹æ³•çš„å¹³å‡å€¼');
      lines.push('');
    }
    
    return lines.join('\n');
  }

  /**
   * æ ¼å¼åŒ–è¾“å‡ºï¼šJSON å½¢å¼
   */
  formatJSON() {
    return JSON.stringify({
      summary: {
        totalFiles: this.results.length,
        totalTokens: this.totalTokens,
        averageTokensPerFile: Math.round(this.totalTokens / this.results.length),
        analysisDate: new Date().toISOString()
      },
      files: this.results,
      methods: {
        simple: 'ä¸­æ–‡å­— Ã—1.2 + è‹±æ–‡è¯ Ã—1.0 + å…¶ä»–',
        tiktoken: 'ä¸­æ–‡å­— Ã—1.5 + è‹±æ–‡è¯ Ã—1.3',
        final: 'ä¸¤ç§æ–¹æ³•çš„å¹³å‡å€¼'
      }
    }, null, 2);
  }

  /**
   * æ ¼å¼åŒ–è¾“å‡ºï¼šCSV å½¢å¼
   */
  formatCSV() {
    const lines = [
      'æ–‡ä»¶å,è¡Œæ•°,å­—ç¬¦æ•°,ä¸­æ–‡å­—æ•°,è‹±æ–‡è¯æ•°,é¢„ä¼°Tokens,å æ¯”(%)'
    ];
    
    this.results.forEach(stat => {
      const percentage = ((stat.tokensEstimated / this.totalTokens) * 100).toFixed(1);
      lines.push(
        `"${stat.file}",${stat.lines},${stat.characters},${stat.chineseChars},${stat.englishWords},${stat.tokensEstimated},${percentage}`
      );
    });
    
    lines.push(`æ€»è®¡,${this.results.reduce((s, r) => s + r.lines, 0)},${this.results.reduce((s, r) => s + r.characters, 0)},${this.results.reduce((s, r) => s + r.chineseChars, 0)},${this.results.reduce((s, r) => s + r.englishWords, 0)},${this.totalTokens},100`);
    
    return lines.join('\n');
  }
}

// ä¸»ç¨‹åº
function main() {
  // è§£æå‘½ä»¤è¡Œå‚æ•°
  const args = process.argv.slice(2);
  const options = {
    format: 'table',
    sort: 'tokens',
    verbose: false,
    threshold: 0
  };
  
  for (let i = 0; i < args.length; i++) {
    if (args[i] === '--format' && args[i + 1]) {
      options.format = args[i + 1];
      i++;
    } else if (args[i] === '--sort' && args[i + 1]) {
      options.sort = args[i + 1];
      i++;
    } else if (args[i] === '--verbose') {
      options.verbose = true;
    } else if (args[i] === '--threshold' && args[i + 1]) {
      options.threshold = parseInt(args[i + 1]);
      i++;
    } else if (args[i] === '--help' || args[i] === '-h') {
      printHelp();
      process.exit(0);
    }
  }
  
  // åˆ†æ worldbooks ç›®å½•
  const worldbooksPath = path.join(__dirname, 'worldbooks');
  
  if (!fs.existsSync(worldbooksPath)) {
    console.error(`âŒ ç›®å½•ä¸å­˜åœ¨: ${worldbooksPath}`);
    process.exit(1);
  }
  
  const analyzer = new TokenAnalyzer();
  analyzer.analyzeDirectory(worldbooksPath)
    .sort(options.sort);
  
  // è¿‡æ»¤é˜ˆå€¼
  if (options.threshold > 0) {
    analyzer.results = analyzer.results.filter(r => r.tokensEstimated >= options.threshold);
  }
  
  // è¾“å‡ºç»“æœ
  let output;
  switch (options.format) {
    case 'json':
      output = analyzer.formatJSON();
      break;
    case 'csv':
      output = analyzer.formatCSV();
      break;
    case 'table':
    default:
      output = analyzer.formatTable(options.verbose);
  }
  
  console.log(output);
}

function printHelp() {
  console.log(`
Token ç»Ÿè®¡åˆ†æå·¥å…·

ä½¿ç”¨æ–¹æ³•:
  node analyze-tokens.js [é€‰é¡¹]

é€‰é¡¹:
  --format <json|table|csv>   è¾“å‡ºæ ¼å¼ (é»˜è®¤: table)
  --sort <file|tokens|lines>  æ’åºæ–¹å¼ (é»˜è®¤: tokens)
  --verbose                   æ˜¾ç¤ºè¯¦ç»†åˆ†æä¿¡æ¯
  --threshold <number>        åªæ˜¾ç¤ºè¶…è¿‡æŒ‡å®štokenæ•°çš„æ–‡ä»¶
  --help, -h                  æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¤ºä¾‹:
  node analyze-tokens.js                    # é»˜è®¤è¡¨æ ¼è¾“å‡ºï¼ŒæŒ‰tokenæ•°æ’åº
  node analyze-tokens.js --format json      # JSON æ ¼å¼è¾“å‡º
  node analyze-tokens.js --verbose          # è¯¦ç»†åˆ†æä¿¡æ¯
  node analyze-tokens.js --threshold 500    # åªæ˜¾ç¤º >500 tokens çš„æ–‡ä»¶
  node analyze-tokens.js --sort lines       # æŒ‰è¡Œæ•°æ’åº
  `);
}

// è¿è¡Œä¸»ç¨‹åº
main();
